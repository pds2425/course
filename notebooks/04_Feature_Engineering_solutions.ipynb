{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/pds2425/course/blob/main/notebooks/04_Feature_Engineering_solutions.ipynb\n",
    "\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<div class='bar_title'></div>\n",
    "\n",
    "*Practical Data Science*\n",
    "\n",
    "# Feature Engineering\n",
    "\n",
    "Gunther Gust & Viet Nguyen<br>\n",
    "Chair for Enterprise AI "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/d3.png\" style=\"width:20%; float:left;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/CAIDASlogo.png\" style=\"width:20%; float:left;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></span></li><li><span><a href=\"#Loading-the-Data\" data-toc-modified-id=\"Loading-the-Data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Loading the Data</a></span></li><li><span><a href=\"#Select-Variables-and-Split-Dataset\" data-toc-modified-id=\"Select-Variables-and-Split-Dataset-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Select Variables and Split Dataset</a></span></li><li><span><a href=\"#Feature-Engineering-on-Numeric-Data\" data-toc-modified-id=\"Feature-Engineering-on-Numeric-Data-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Feature Engineering on Numeric Data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Preprocessing\" data-toc-modified-id=\"Preprocessing-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Preprocessing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Normalization\" data-toc-modified-id=\"Normalization-4.1.1\"><span class=\"toc-item-num\">4.1.1&nbsp;&nbsp;</span>Normalization</a></span></li><li><span><a href=\"#Standardization\" data-toc-modified-id=\"Standardization-4.1.2\"><span class=\"toc-item-num\">4.1.2&nbsp;&nbsp;</span>Standardization</a></span></li><li><span><a href=\"#Summary\" data-toc-modified-id=\"Summary-4.1.3\"><span class=\"toc-item-num\">4.1.3&nbsp;&nbsp;</span>Summary</a></span></li></ul></li><li><span><a href=\"#Binarization\" data-toc-modified-id=\"Binarization-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Binarization</a></span></li><li><span><a href=\"#Binning\" data-toc-modified-id=\"Binning-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Binning</a></span><ul class=\"toc-item\"><li><span><a href=\"#Fixed-Width-Binning\" data-toc-modified-id=\"Fixed-Width-Binning-4.3.1\"><span class=\"toc-item-num\">4.3.1&nbsp;&nbsp;</span>Fixed-Width Binning</a></span></li><li><span><a href=\"#Adaptive-Binning\" data-toc-modified-id=\"Adaptive-Binning-4.3.2\"><span class=\"toc-item-num\">4.3.2&nbsp;&nbsp;</span>Adaptive Binning</a></span></li></ul></li><li><span><a href=\"#Statistical-Transformations\" data-toc-modified-id=\"Statistical-Transformations-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Statistical Transformations</a></span></li><li><span><a href=\"#Evaluation\" data-toc-modified-id=\"Evaluation-4.5\"><span class=\"toc-item-num\">4.5&nbsp;&nbsp;</span>Evaluation</a></span></li></ul></li><li><span><a href=\"#Feature-Engineering-on-Categorical-Data\" data-toc-modified-id=\"Feature-Engineering-on-Categorical-Data-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Feature Engineering on Categorical Data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Label-and-One-Hot-Encoding\" data-toc-modified-id=\"Label-and-One-Hot-Encoding-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Label and One-Hot-Encoding</a></span></li><li><span><a href=\"#Count-Encodings\" data-toc-modified-id=\"Count-Encodings-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Count Encodings</a></span></li><li><span><a href=\"#Target-Encodings\" data-toc-modified-id=\"Target-Encodings-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Target Encodings</a></span></li><li><span><a href=\"#CatBoost-Encoding\" data-toc-modified-id=\"CatBoost-Encoding-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>CatBoost Encoding</a></span></li><li><span><a href=\"#Warning\" data-toc-modified-id=\"Warning-5.5\"><span class=\"toc-item-num\">5.5&nbsp;&nbsp;</span>Warning</a></span></li></ul></li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Conclusion</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Credits__\n",
    "\n",
    "Parts of the material of this lecture are adopted from www.kaggle.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**This lecture provides an overview on different feature engineering techniques.**\n",
    "\n",
    "Starting with a baseline dataset, we will\n",
    "\n",
    "- modify existing variables \n",
    "- add additional features to  our dataset \n",
    "- train a predictive model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Feature engineering** is an essential part of building a powerful predictive model. \n",
    "\n",
    "Each problem is domain specific and better features (suited to the problem) are often the deciding factor of the performance of your system. \n",
    "\n",
    "Feature Engineering requires experience as well as creativity and this is the reason **Data Scientists often spend the majority of their time** in the data preparation phase before modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "_\"Coming up with features is difficult, time-consuming, requires expert knowledge. Applied machine learning is basically feature engineering.\"_\n",
    "\n",
    "Prof. Andrew Ng."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "_\"Feature engineering is the process of transforming raw data into features that better represent the underlying problem to the predictive models, resulting in improved model accuracy on unseen data.\"_\n",
    "\n",
    "Dr. Jason Brownlee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "_\"At the end of the day, some machine learning projects succeed and some fail. What makes the difference? Easily the most important factor is the features used.\"_\n",
    "\n",
    "Prof. Pedro Domingos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Loading the Data\n",
    "This week, we will work with a sample of the [adult dataset](https://archive.ics.uci.edu/ml/datasets/adult) which has some __census information on individuals__. We'll use it to train a model to predict whether __salary is greater than 50k USD or not.__ Again, our first step is to load and familiarize ourself with the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>49</td>\n",
       "      <td>Private</td>\n",
       "      <td>101320</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>12.0</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wife</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>1902</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;=50k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44</td>\n",
       "      <td>Private</td>\n",
       "      <td>236746</td>\n",
       "      <td>Masters</td>\n",
       "      <td>14.0</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>10520</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;=50k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>96185</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;50k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>38</td>\n",
       "      <td>Self-emp-inc</td>\n",
       "      <td>112847</td>\n",
       "      <td>Prof-school</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Asian-Pac-Islander</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;=50k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>42</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>82297</td>\n",
       "      <td>7th-8th</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;50k</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age          workclass  fnlwgt     education  education-num  \\\n",
       "0   49            Private  101320    Assoc-acdm           12.0   \n",
       "1   44            Private  236746       Masters           14.0   \n",
       "2   38            Private   96185       HS-grad            NaN   \n",
       "3   38       Self-emp-inc  112847   Prof-school           15.0   \n",
       "4   42   Self-emp-not-inc   82297       7th-8th            NaN   \n",
       "\n",
       "        marital-status        occupation    relationship                 race  \\\n",
       "0   Married-civ-spouse               NaN            Wife                White   \n",
       "1             Divorced   Exec-managerial   Not-in-family                White   \n",
       "2             Divorced               NaN       Unmarried                Black   \n",
       "3   Married-civ-spouse    Prof-specialty         Husband   Asian-Pac-Islander   \n",
       "4   Married-civ-spouse     Other-service            Wife                Black   \n",
       "\n",
       "       sex  capital-gain  capital-loss  hours-per-week  native-country salary  \n",
       "0   Female             0          1902              40   United-States  >=50k  \n",
       "1     Male         10520             0              45   United-States  >=50k  \n",
       "2   Female             0             0              32   United-States   <50k  \n",
       "3     Male             0             0              40   United-States  >=50k  \n",
       "4   Female             0             0              50   United-States   <50k  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = 'https://raw.githubusercontent.com/GuntherGust/tds2_data/main/data/adult.csv'\n",
    "adult_data = pd.read_csv(file_path)\n",
    "adult_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Select Variables and Split Dataset\n",
    "\n",
    "Before we start to engineer new features, we select the feature and target variables. \n",
    "\n",
    "The (binary) variable ``salary`` describes if a person earns more or less that \\\\$50k. We replace the labels with numeric values (0: Salary < \\\\$50k, 1: Salary > \\\\$50k) and subsequently select it as our target variable y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age          workclass  fnlwgt     education  education-num  \\\n",
      "0   49            Private  101320    Assoc-acdm           12.0   \n",
      "1   44            Private  236746       Masters           14.0   \n",
      "2   38            Private   96185       HS-grad            NaN   \n",
      "3   38       Self-emp-inc  112847   Prof-school           15.0   \n",
      "4   42   Self-emp-not-inc   82297       7th-8th            NaN   \n",
      "\n",
      "        marital-status        occupation    relationship                 race  \\\n",
      "0   Married-civ-spouse               NaN            Wife                White   \n",
      "1             Divorced   Exec-managerial   Not-in-family                White   \n",
      "2             Divorced               NaN       Unmarried                Black   \n",
      "3   Married-civ-spouse    Prof-specialty         Husband   Asian-Pac-Islander   \n",
      "4   Married-civ-spouse     Other-service            Wife                Black   \n",
      "\n",
      "       sex  capital-gain  capital-loss  hours-per-week  native-country  salary  \n",
      "0   Female             0          1902              40   United-States       1  \n",
      "1     Male         10520             0              45   United-States       1  \n",
      "2   Female             0             0              32   United-States       0  \n",
      "3     Male             0             0              40   United-States       1  \n",
      "4   Female             0             0              50   United-States       0  \n"
     ]
    }
   ],
   "source": [
    "adult_data = adult_data.assign(salary=(adult_data['salary']=='>=50k').astype(int))\n",
    "print(adult_data.head())\n",
    "\n",
    "y = adult_data['salary']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The remaining columns serve as our features X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X = adult_data.drop('salary', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Next, we perform a train-test split to train and evaluate our machine learning models for the model validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now we are ready to start preparing and enhancing our numerical and categorical features!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Feature Engineering on Numeric Data\n",
    "\n",
    "By Numeric data we mean continuous data and not discrete data which is typically represented as categorical data. Integers and floats are the most common and widely used numeric data types for continuous numeric data. Even though numeric data can be directly fed into machine learning models, we still have to engineer and preprocess features which are relevant to the scenario, problem, domain and machine learning model.\n",
    "\n",
    "To this end, we can distinguish between preprocessing and feature generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "To work on our numeric features, we have to identify all numeric columns in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age',\n",
       " 'fnlwgt',\n",
       " 'education-num',\n",
       " 'capital-gain',\n",
       " 'capital-loss',\n",
       " 'hours-per-week']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numCols = [cname for cname in train_X.columns if train_X[cname].dtype != \"object\"]\n",
    "numCols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To avoid problems with missing values we use a ``SimpleImputer`` for the numeric columns before we continue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "simple_imputer = SimpleImputer()\n",
    "\n",
    "train_X_num = pd.DataFrame(simple_imputer.fit_transform(train_X[numCols]), columns=numCols, index=train_X.index)\n",
    "val_X_num = pd.DataFrame(simple_imputer.transform(val_X[numCols]), columns=numCols, index=val_X.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Preprocessing\n",
    "\n",
    "Our dataset may contain attributes with a mixture of scales for various quantities. However, many machine learning methods require or at least are more effective if the data attributes have the same scale. \n",
    "\n",
    "For example, ``capital gain`` and ``capital loss`` is measured in USD while age is measured in years in our dataset at hand.\n",
    "\n",
    "To avoid having numeric values from different scales we can use two popular data scaling methods: normalization and standardization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Normalization\n",
    "\n",
    "Normalization refers to rescaling numeric attributes into the __range between 0 and 1.__ It is useful to scale the input attributes for a model that relies on the magnitude of values, such as distance measures used in k-nearest neighbors and in the preparation of coefficients in regression.\n",
    "\n",
    "Using Scikit-learn's ``MinMaxScaler`` we can rescale an attribute according to the following formula:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "    X = \\frac{(X - min(X))}{(max(X) - min(X))}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education-num</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26464</th>\n",
       "      <td>0.136986</td>\n",
       "      <td>0.215102</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16134</th>\n",
       "      <td>0.054795</td>\n",
       "      <td>0.118419</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.193878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4747</th>\n",
       "      <td>0.465753</td>\n",
       "      <td>0.038438</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.346939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8369</th>\n",
       "      <td>0.095890</td>\n",
       "      <td>0.128628</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.602041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5741</th>\n",
       "      <td>0.205479</td>\n",
       "      <td>0.064474</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13123</th>\n",
       "      <td>0.041096</td>\n",
       "      <td>0.093175</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.244898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19648</th>\n",
       "      <td>0.205479</td>\n",
       "      <td>0.176642</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9845</th>\n",
       "      <td>0.315068</td>\n",
       "      <td>0.039635</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.193878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10799</th>\n",
       "      <td>0.150685</td>\n",
       "      <td>0.011654</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2732</th>\n",
       "      <td>0.109589</td>\n",
       "      <td>0.142264</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.03325</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.448980</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24420 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            age    fnlwgt  education-num  capital-gain  capital-loss  \\\n",
       "26464  0.136986  0.215102       0.533333       0.00000           0.0   \n",
       "16134  0.054795  0.118419       0.600000       0.00000           0.0   \n",
       "4747   0.465753  0.038438       0.066667       0.00000           0.0   \n",
       "8369   0.095890  0.128628       0.533333       0.00000           0.0   \n",
       "5741   0.205479  0.064474       0.800000       0.00000           0.0   \n",
       "...         ...       ...            ...           ...           ...   \n",
       "13123  0.041096  0.093175       0.600000       0.00000           0.0   \n",
       "19648  0.205479  0.176642       0.733333       0.00000           0.0   \n",
       "9845   0.315068  0.039635       0.000000       0.00000           0.0   \n",
       "10799  0.150685  0.011654       0.533333       0.00000           0.0   \n",
       "2732   0.109589  0.142264       0.533333       0.03325           0.0   \n",
       "\n",
       "       hours-per-week  \n",
       "26464        0.397959  \n",
       "16134        0.193878  \n",
       "4747         0.346939  \n",
       "8369         0.602041  \n",
       "5741         0.397959  \n",
       "...               ...  \n",
       "13123        0.244898  \n",
       "19648        0.397959  \n",
       "9845         0.193878  \n",
       "10799        0.397959  \n",
       "2732         0.448980  \n",
       "\n",
       "[24420 rows x 6 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "train_X_num_normalized = pd.DataFrame(scaler.fit_transform(train_X_num), \n",
    "                                      columns=train_X_num.columns, index=train_X_num.index)\n",
    "val_X_num_normalized = pd.DataFrame(scaler.transform(val_X_num), \n",
    "                                    columns=train_X_num.columns, index=val_X_num.index)\n",
    "\n",
    "train_X_num_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Standardization\n",
    "\n",
    "In contrast to normalization, we could also use standardization for our numerical variables. In this context, standardization refers to shifting the distribution of each attribute to have a mean of zero and a standard deviation of one. It is useful to standardize attributes for a model that relies on the distribution of attributes such as Gaussian processes.\n",
    "\n",
    "Using Scikit-learn's ```StandardScaler``` we can rescale an attribute according to the following formula:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "    X = \\frac{(X - mean(X))}{\\sqrt{var(X)}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education-num</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26464</th>\n",
       "      <td>-0.853017</td>\n",
       "      <td>1.316410</td>\n",
       "      <td>-0.420746</td>\n",
       "      <td>-0.146391</td>\n",
       "      <td>-0.217388</td>\n",
       "      <td>-0.036400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16134</th>\n",
       "      <td>-1.292554</td>\n",
       "      <td>-0.027373</td>\n",
       "      <td>-0.029346</td>\n",
       "      <td>-0.146391</td>\n",
       "      <td>-0.217388</td>\n",
       "      <td>-1.652429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4747</th>\n",
       "      <td>0.905131</td>\n",
       "      <td>-1.139029</td>\n",
       "      <td>-3.160546</td>\n",
       "      <td>-0.146391</td>\n",
       "      <td>-0.217388</td>\n",
       "      <td>-0.440408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8369</th>\n",
       "      <td>-1.072786</td>\n",
       "      <td>0.114522</td>\n",
       "      <td>-0.420746</td>\n",
       "      <td>-0.146391</td>\n",
       "      <td>-0.217388</td>\n",
       "      <td>1.579628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5741</th>\n",
       "      <td>-0.486737</td>\n",
       "      <td>-0.777155</td>\n",
       "      <td>1.144853</td>\n",
       "      <td>-0.146391</td>\n",
       "      <td>-0.217388</td>\n",
       "      <td>-0.036400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            age    fnlwgt  education-num  capital-gain  capital-loss  \\\n",
       "26464 -0.853017  1.316410      -0.420746     -0.146391     -0.217388   \n",
       "16134 -1.292554 -0.027373      -0.029346     -0.146391     -0.217388   \n",
       "4747   0.905131 -1.139029      -3.160546     -0.146391     -0.217388   \n",
       "8369  -1.072786  0.114522      -0.420746     -0.146391     -0.217388   \n",
       "5741  -0.486737 -0.777155       1.144853     -0.146391     -0.217388   \n",
       "\n",
       "       hours-per-week  \n",
       "26464       -0.036400  \n",
       "16134       -1.652429  \n",
       "4747        -0.440408  \n",
       "8369         1.579628  \n",
       "5741        -0.036400  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "train_X_num_standardized = pd.DataFrame(scaler.fit_transform(train_X_num), \n",
    "                                        columns=train_X_num.columns, index=train_X_num.index)\n",
    "val_X_num_standardized = pd.DataFrame(scaler.transform(val_X_num), \n",
    "                                      columns=train_X_num.columns, index=val_X_num.index)\n",
    "\n",
    "train_X_num_standardized.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Summary\n",
    "\n",
    "Data rescaling is an important part of data preparation before applying machine learning algorithms. However, it is hard to know whether normalization or standardization of the data will improve the performance of a predictive model in advance. \n",
    "\n",
    "A good tip for a practical application is to create rescaled copies of your dataset and evaluate them against each other. This process can quickly show which rescaling method will improve your selected models in the problem at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Binarization\n",
    "\n",
    "For some problems raw frequencies or counts may not be relevant for building a model. In these cases it is only relevant if a numeric value exceeds a specific threshold (e.g. a person is at least 40 years old). Hence we do not require the number of times the action was performed but only a binary feature.\n",
    "\n",
    "We can binarize a feature using Scikit-learn's ``Binarizer`` function (Note that we use the raw dataset for this example - clearly we could normalize or standardize the dataframe afterwards):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education-num</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>40Plus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26464</th>\n",
       "      <td>27.0</td>\n",
       "      <td>329005.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16134</th>\n",
       "      <td>21.0</td>\n",
       "      <td>186648.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4747</th>\n",
       "      <td>51.0</td>\n",
       "      <td>68882.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8369</th>\n",
       "      <td>24.0</td>\n",
       "      <td>201680.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5741</th>\n",
       "      <td>32.0</td>\n",
       "      <td>107218.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        age    fnlwgt  education-num  capital-gain  capital-loss  \\\n",
       "26464  27.0  329005.0            9.0           0.0           0.0   \n",
       "16134  21.0  186648.0           10.0           0.0           0.0   \n",
       "4747   51.0   68882.0            2.0           0.0           0.0   \n",
       "8369   24.0  201680.0            9.0           0.0           0.0   \n",
       "5741   32.0  107218.0           13.0           0.0           0.0   \n",
       "\n",
       "       hours-per-week  40Plus  \n",
       "26464            40.0     0.0  \n",
       "16134            20.0     0.0  \n",
       "4747             35.0     1.0  \n",
       "8369             60.0     0.0  \n",
       "5741             40.0     0.0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import Binarizer\n",
    "\n",
    "train_X_binary_age = train_X_num.copy()\n",
    "val_X_binary_age = val_X_num.copy()\n",
    "\n",
    "binarizer = Binarizer(threshold=40)\n",
    "\n",
    "train_X_binary_age['40Plus'] = binarizer.transform([train_X_binary_age['age']])[0]\n",
    "val_X_binary_age['40Plus'] = binarizer.transform([val_X_binary_age['age']])[0]\n",
    "\n",
    "train_X_binary_age.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Binning\n",
    "\n",
    "The problem of working with raw, numeric features is that often the distribution of values in these features will be skewed. This signifies that some values will occur quite frequently while some will be quite rare. Hence there are strategies to deal with this, which include binning. \n",
    "\n",
    "Binning is used for transforming continuous numeric features into discrete ones. These discrete values can be interpreted as categories or bins into which the raw values are grouped into. Each group represents a specific degree of intensity and hence a specific range of continuous numeric values fall into it.\n",
    "\n",
    "Let's again use the age variable to perform two different types of binning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Fixed-Width Binning\n",
    "\n",
    "In fixed-width binning, specific fixed widths for each bin are defined by the user. Each bin has a fixed range of values which should be assigned to that bin on the basis of some domain knowledge.\n",
    "\n",
    "We can use Pandas ```cut``` function to bin the age into predefined groups and assign labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education-num</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>AgeBinned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26464</th>\n",
       "      <td>27.0</td>\n",
       "      <td>329005.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16134</th>\n",
       "      <td>21.0</td>\n",
       "      <td>186648.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4747</th>\n",
       "      <td>51.0</td>\n",
       "      <td>68882.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8369</th>\n",
       "      <td>24.0</td>\n",
       "      <td>201680.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5741</th>\n",
       "      <td>32.0</td>\n",
       "      <td>107218.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        age    fnlwgt  education-num  capital-gain  capital-loss  \\\n",
       "26464  27.0  329005.0            9.0           0.0           0.0   \n",
       "16134  21.0  186648.0           10.0           0.0           0.0   \n",
       "4747   51.0   68882.0            2.0           0.0           0.0   \n",
       "8369   24.0  201680.0            9.0           0.0           0.0   \n",
       "5741   32.0  107218.0           13.0           0.0           0.0   \n",
       "\n",
       "       hours-per-week AgeBinned  \n",
       "26464            40.0         1  \n",
       "16134            20.0         0  \n",
       "4747             35.0         1  \n",
       "8369             60.0         0  \n",
       "5741             40.0         1  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X_bin_age = train_X_num.copy()\n",
    "val_X_bin_age = val_X_num.copy()\n",
    "\n",
    "bin_ranges = [0, 25, 60, 999]\n",
    "bin_labels = [0, 1, 2]\n",
    "\n",
    "train_X_bin_age['AgeBinned'] = pd.cut(train_X_bin_age['age'], \n",
    "                                      bins=bin_ranges, labels=bin_labels)\n",
    "val_X_bin_age['AgeBinned'] = pd.cut(val_X_bin_age['age'], \n",
    "                                    bins=bin_ranges, labels=bin_labels)\n",
    "\n",
    "train_X_bin_age.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Adaptive Binning\n",
    "\n",
    "The major drawback in using fixed-width binning is __unbalanced bin sizes.__ As we manually decide the bin ranges, we can end up with irregular bins which are not uniform based on the number of data points. Some bins (such as \"young (0)\" and \"old (2)\") might be sparsely populated while some (such as \"medium (1)\") are densely populated.\n",
    "\n",
    "To overcome this issues we can use adaptive binning based on the distribution of the data.\n",
    "\n",
    "To cut the space into equal partitions we can use the quantiles as cut-points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education-num</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>AgeBinned</th>\n",
       "      <th>AgeBinnedAdaptive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26464</th>\n",
       "      <td>27.0</td>\n",
       "      <td>329005.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16134</th>\n",
       "      <td>21.0</td>\n",
       "      <td>186648.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4747</th>\n",
       "      <td>51.0</td>\n",
       "      <td>68882.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8369</th>\n",
       "      <td>24.0</td>\n",
       "      <td>201680.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5741</th>\n",
       "      <td>32.0</td>\n",
       "      <td>107218.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        age    fnlwgt  education-num  capital-gain  capital-loss  \\\n",
       "26464  27.0  329005.0            9.0           0.0           0.0   \n",
       "16134  21.0  186648.0           10.0           0.0           0.0   \n",
       "4747   51.0   68882.0            2.0           0.0           0.0   \n",
       "8369   24.0  201680.0            9.0           0.0           0.0   \n",
       "5741   32.0  107218.0           13.0           0.0           0.0   \n",
       "\n",
       "       hours-per-week AgeBinned AgeBinnedAdaptive  \n",
       "26464            40.0         1                 0  \n",
       "16134            20.0         0                 0  \n",
       "4747             35.0         1                 2  \n",
       "8369             60.0         0                 0  \n",
       "5741             40.0         1                 1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantile_list = [0, 0.33, 0.66, 1]\n",
    "quantile_labels = [0, 1, 2]\n",
    "\n",
    "train_X_bin_age['AgeBinnedAdaptive'] = pd.qcut(train_X_bin_age['age'], \n",
    "                                               q=quantile_list, labels=quantile_labels)\n",
    "val_X_bin_age['AgeBinnedAdaptive'] = pd.qcut(val_X_bin_age['age'],   \n",
    "                                             q=quantile_list, labels=quantile_labels)\n",
    "\n",
    "train_X_bin_age.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Statistical Transformations\n",
    "\n",
    "Many variables, such as ``capital-gain`` or ``fnlwgt`` (sampling weight) span several orders of magnitude. While the vast majority of persons has very small capital-gains, a few people have very high gains. To work with such skewed variables we can use the log transformation. \n",
    "\n",
    "__Log transforms__ are useful when applied to skewed distributions as they tend to expand the values which fall in the range of lower magnitudes and tend to compress or reduce the values which fall in the range of higher magnitudes. This tends to make the skewed distribution as normal-like as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_X_logGains = train_X_num.copy()\n",
    "val_X_logGains = val_X_num.copy()\n",
    "\n",
    "train_X_logGains['logfnlwgt'] = np.log1p(train_X_logGains['fnlwgt'])\n",
    "val_X_logGains['logfnlwgt'] = np.log1p(val_X_logGains['fnlwgt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can see this effect plotting both histograms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "train_X_logGains[['fnlwgt', 'logfnlwgt']].hist();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Evaluation\n",
    "\n",
    "We can train support vector machines (support vector classifiers, ``SVC``) using the different datasets and feature engineering techniques to evaluate their impact on the model performance. Note that we could (and should) combine these techniques to train powerful models and apply them in real-world problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def score_dataset(X_train, X_valid, y_train, y_valid):\n",
    "    model = SVC(gamma='auto', random_state=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return accuracy_score(y_valid, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Features: 0.7558039552880481\n",
      "Normalized Features: 0.800761577201818\n",
      "Standardized Features: 0.8204151824100233\n",
      "Binary Age: 0.7560496253531507\n",
      "Binned Age: 0.7560496253531507\n",
      "Log FNLWGT: 0.7560496253531507\n"
     ]
    }
   ],
   "source": [
    "print(\"Raw Features: {}\".\n",
    "      format(score_dataset(train_X_num, val_X_num, train_y, val_y)))\n",
    "print(\"Normalized Features: {}\".\n",
    "      format(score_dataset(train_X_num_normalized, val_X_num_normalized, train_y, val_y)))\n",
    "print(\"Standardized Features: {}\".\n",
    "      format(score_dataset(train_X_num_standardized, val_X_num_standardized, train_y, val_y)))\n",
    "print(\"Binary Age: {}\".format(score_dataset(train_X_binary_age, val_X_binary_age, train_y, val_y)))\n",
    "print(\"Binned Age: {}\".format(score_dataset(train_X_bin_age, val_X_bin_age, train_y, val_y)))\n",
    "print(\"Log FNLWGT: {}\".format(score_dataset(train_X_logGains, val_X_logGains, train_y, val_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "1. Replace `SVC` model with the `RandomForestClassifier` model and retrain the model with: Raw Features, Normalized Features and Standardized Features. Hint: import the model in this [scikit-learn documentation](https://scikit-learn.org/1.5/modules/generated/sklearn.ensemble.RandomForestClassifier.html). You only need to set `n_estimators=100` and `random_state=1`.\n",
    "\n",
    "2. Can you explain why the results are relatively the same?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Features: 0.806411988699177\n",
      "Normalized Features: 0.8061663186340744\n",
      "Standardized Features: 0.8056749785038693\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def score_dataset_rf(X_train, X_valid, y_train, y_valid):\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=1)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return accuracy_score(y_valid, preds)\n",
    "\n",
    "print(\"Raw Features: {}\".\n",
    "      format(score_dataset_rf(train_X_num, val_X_num, train_y, val_y)))\n",
    "print(\"Normalized Features: {}\".\n",
    "      format(score_dataset_rf(train_X_num_normalized, val_X_num_normalized, train_y, val_y)))\n",
    "print(\"Standardized Features: {}\".\n",
    "      format(score_dataset_rf(train_X_num_standardized, val_X_num_standardized, train_y, val_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling techniques aim to put all features into the same scale, preventing any single feature from dominating others in priority. These techniques are crucial for distance-based algorithms that rely on distance measures for their calculations such as K-Nearest-Neighbors or Support Vector Machines. On the other hand, Random Forest is a tree-based modelm not a distance-based one. The algorithm partitions data to make predictions, and each partition (or split) of a decision tree is soly based on one feature, which is independent of other features. Therefore, scaling is unncessary for tree-based algorithms. For more in-depth explanation, check the [resource here](https://www.kdnuggets.com/2022/07/random-forest-algorithm-need-normalization.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Feature Engineering on Categorical Data\n",
    "\n",
    "In contrast to continuous numeric data we mean discrete values which belong to a specific finite set of categories or classes when we talk about categorical data. These discrete values can be text or numeric in nature and there are two major classes of categorical data, nominal and ordinal.\n",
    "\n",
    "While a lot of advancements have been made in state of the art machine learning frameworks to accept categorical data types like text labels. Typically any standard workflow in feature engineering involves some form of transformation of these categorical values into numeric labels and then applying some encoding scheme on these values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### One-Hot-Encoding\n",
    "\n",
    "Last week, we already talked about label and one-hot-encoding to prepare our categorical features for machine learning models. To get started, we will impute missing values and encode all categorical features using the ``OneHotEncoder``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Again, we will use a helper function to evaluate the performance of our models. This time, we will rely on a random forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "catCols = [cname for cname in train_X.columns if train_X[cname].dtype == \"object\"]\n",
    "\n",
    "train_X_cat = train_X[catCols].copy()\n",
    "val_X_cat = val_X[catCols].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "train_X_imputed = pd.DataFrame(simple_imputer.fit_transform(train_X_cat), columns=train_X_cat.columns, index=train_X_cat.index)\n",
    "val_X_imputed  = pd.DataFrame(simple_imputer.transform(val_X_cat), columns=val_X_cat.columns, index=val_X_cat.index)\n",
    "\n",
    "oh_encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "train_X_OHenc = pd.DataFrame(oh_encoder.fit_transform(train_X_imputed), index=train_X_imputed.index)\n",
    "val_X_OHenc = pd.DataFrame(oh_encoder.transform(val_X_imputed), index=val_X_imputed.index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def score_dataset(X_train, X_valid, y_train, y_valid):\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return accuracy_score(y_valid, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "To evaluate the model we combine the raw numerical data and the encoded categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-Hot encoded categorical + raw numeric: 0.8523522908733571\n"
     ]
    }
   ],
   "source": [
    "train_X_OH_num = train_X_num.join(train_X_OHenc.add_suffix(\"_OHenc\"))\n",
    "val_X_OH_num = val_X_num.join(val_X_OHenc.add_suffix(\"_OHenc\"))\n",
    "\n",
    "\n",
    "print(\"One-Hot encoded categorical + raw numeric: {}\".\n",
    "      format(score_dataset(train_X_OH_num, val_X_OH_num, train_y, val_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Count Encodings\n",
    "\n",
    "While label and one-hot encoding often yield good results, there are also a lot of other (more complex) techniques to encode categorical variables. The package [categorical-encoding](https://github.com/scikit-learn-contrib/categorical-encoding) offers implementations of many different techniques.\n",
    "\n",
    "One prominent variant is called count encoding. Count encoding replaces each categorical value with the number of times it appears in the dataset. For example, if the value \"USA\" occures 50 times in the country feature, then each \"USA\" would be replaced with the number 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count encoded categorical + raw numeric: 0.8500184252548827\n"
     ]
    }
   ],
   "source": [
    "from category_encoders import CountEncoder\n",
    "\n",
    "count_encoder = CountEncoder(handle_unknown=0, handle_missing='value')\n",
    "\n",
    "train_X_countenc = count_encoder.fit_transform(train_X_cat)\n",
    "val_X_countenc = count_encoder.transform(val_X_cat)\n",
    "\n",
    "train_X_count_num = train_X_num.join(train_X_countenc.add_suffix(\"_countenc\"))\n",
    "val_X_count_num = val_X_num.join(val_X_countenc.add_suffix(\"_countenc\"))\n",
    "\n",
    "print(\"Count encoded categorical + raw numeric: {}\".\n",
    "      format(score_dataset(train_X_count_num, val_X_count_num, train_y, val_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Target Encodings\n",
    "\n",
    "Target encoding is another advanced (but sometimes dangerous) approach to encode categorical features. It replaces a categorical value with the average value of the target for that value of the feature. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For example, given the country value \"GER\", you'd calculate the average outcome for all the rows with country == 'GER'. This value is often blended with the target probability over the entire dataset to reduce the variance of values with few occurences and to avoid overfitting. See [here](https://towardsdatascience.com/dealing-with-categorical-variables-by-using-target-encoder-a0f1733a4c69) for a more detailed explanation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__Attention:__ This technique uses the targets to create new features. So including the validation or test data in the target encodings would be a form of target leakage. Instead, you should learn the target encodings from the __training dataset only__ and apply it to the other datasets (as we did with all other encoding methods).\n",
    "\n",
    "See also [here](https://medium.com/analytics-vidhya/target-encoding-vs-one-hot-encoding-with-simple-examples-276a7e7b3e64) for another illustrative example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target encoded categorical + raw numeric: 0.8524751259059083\n"
     ]
    }
   ],
   "source": [
    "from category_encoders import TargetEncoder\n",
    "\n",
    "target_encoder = TargetEncoder()\n",
    "\n",
    "train_X_targetenc = target_encoder.fit_transform(train_X_cat, train_y)\n",
    "val_X_targetenc = target_encoder.transform(val_X_cat)\n",
    "\n",
    "train_X_target_num = train_X_num.join(train_X_targetenc.add_suffix(\"_targetenc\"))\n",
    "val_X_target_num = val_X_num.join(val_X_targetenc.add_suffix(\"_targetenc\"))\n",
    "\n",
    "print(\"Target encoded categorical + raw numeric: {}\".\n",
    "      format(score_dataset(train_X_target_num, val_X_target_num, train_y, val_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### CatBoost Encoding\n",
    "\n",
    "Finally, we'll look at CatBoost encoding. CatBoost extends the target encoding approach. It reduces problems of target encoding regarding target leakage and overfitting.\n",
    "\n",
    "Roughtly, Catboost encoding is similar to target encoding in that it's based on the target probablity for a given value. However with CatBoost, for each row, the __target probability__ is calculated only from __the rows before it__. Interested readers may find more details [here](https://towardsdatascience.com/how-catboost-encodes-categorical-variables-3866fb2ae640). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoost encoded categorical + raw numeric: 0.860090897924088\n"
     ]
    }
   ],
   "source": [
    "from category_encoders import CatBoostEncoder\n",
    "\n",
    "catboost_encoder = CatBoostEncoder()\n",
    "\n",
    "train_X_catboostenc = catboost_encoder.fit_transform(train_X_cat, train_y)\n",
    "val_X_catboostenc = catboost_encoder.transform(val_X_cat)\n",
    "\n",
    "train_X_catboost_num = train_X_num.join(train_X_catboostenc.add_suffix(\"_targetenc\"))\n",
    "val_X_catboost_num = val_X_num.join(val_X_catboostenc.add_suffix(\"_targetenc\"))\n",
    "\n",
    "print(\"CatBoost encoded categorical + raw numeric: {}\".\n",
    "      format(score_dataset(train_X_catboost_num, val_X_catboost_num, train_y, val_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Warning\n",
    "\n",
    "Target encoding is a powerful but dangerous way to improve on your machine learning methods. \n",
    "\n",
    "Advantages: \n",
    "* Compact transformation of categorical variables\n",
    "* Powerful basis for feature engineering\n",
    "\n",
    "Disadvantages:\n",
    "* Careful validation is required to avoid overfitting\n",
    "* Significant performance improvements only on some datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "1. Rewrite the score_dataset to use Support Vector Classifier `SVC` and train the case of CatBoost Encoding again with `SVC`. Hint: you can use the provided `score_dataset_svc` below, which is copied from the section \"Feature Engineering on Numeric Data\" above.\n",
    "\n",
    "2. Use standardized numeric features, i.e. `train_X_num_standardized` and `val_X_num_standardized` from previous section and combine it with CatBoost Encoding for training the `SVC` model.\n",
    "\n",
    "The final results should have two cases: \n",
    "\n",
    "- One-hot encoded categorical + standardized numeric\n",
    "- CatBoost encoded categorical + standardized numeric\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Use this for the exercises\n",
    "def score_dataset_svc(X_train, X_valid, y_train, y_valid):\n",
    "    model = SVC(gamma='auto', random_state=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return accuracy_score(y_valid, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target encoded categorical + standardized numeric: 0.8506326004176391\n"
     ]
    }
   ],
   "source": [
    "# Join the standardized features of train & validation sets with the one-hot encoded categorical features \n",
    "train_X_onehot_num_std = train_X_num_standardized.join(train_X_OHenc.add_suffix(\"_targetenc\"))\n",
    "val_X_onehot_num_std = val_X_num_standardized.join(val_X_OHenc.add_suffix(\"_targetenc\"))\n",
    "\n",
    "# Train CatBoost encoded categorical + standardized numeric\n",
    "print(\"Target encoded categorical + standardized numeric: {}\".\n",
    "      format(score_dataset_svc(train_X_onehot_num_std, val_X_onehot_num_std, train_y, val_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoost encoded categorical + standardized numeric: 0.8487900749293699\n"
     ]
    }
   ],
   "source": [
    "# Join the standardized features of train & validation sets with the CatBoost encoded categorical features \n",
    "train_X_catboost_num_std = train_X_num_standardized.join(train_X_catboostenc.add_suffix(\"_targetenc\"))\n",
    "val_X_catboost_num_std = val_X_num_standardized.join(val_X_catboostenc.add_suffix(\"_targetenc\"))\n",
    "\n",
    "# Train CatBoost encoded categorical + standardized numeric\n",
    "print(\"CatBoost encoded categorical + standardized numeric: {}\".\n",
    "      format(score_dataset_svc(train_X_catboost_num_std, val_X_catboost_num_std, train_y, val_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mentimeters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "Today, we have seen a variety of ways to encode numerical and categorical features to improve the performance of our machine learning models. To try even more encoding methods you can try the implementations in the categorical-encoding package on [github](https://github.com/scikit-learn-contrib/categorical-encoding).\n",
    "\n",
    "While the approaches we have talked about today have the potential to create powerful models, they require a lot of manual tuning and testing. "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "rise": {
   "enable_chalkboard": false,
   "overlay": "<div class='background'></div><div class='header'>WS 23/24</br>TDS2</div><div class='logo'><img src='images/d3logo.png'></div><div class='bar'></div>",
   "scroll": true,
   "slideNumber": "h.v"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
